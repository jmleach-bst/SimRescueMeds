@article{Kelter2024,
	abstract = {Statistical simulation studies are becoming increasingly popular to demonstrate the performance or superiority of new computational procedures and algorithms. Despite this status quo, previous surveys of the literature have shown that the reporting of statistical simulation studies often lacks relevant information and structure. The latter applies in particular to Bayesian simulation studies, and in this paper the Bayesian simulation study framework (BASIS) is presented as a step towards improving the situation. The BASIS framework provides a structured skeleton for planning, coding, executing, analyzing, and reporting Bayesian simulation studies in biometrical research and computational statistics. It encompasses various features of previous proposals and recommendations in the methodological literature and aims to promote neutral comparison studies in statistical research. Computational aspects covered in the BASIS include algorithmic choices, Markov–chain-Monte-Carlo convergence diagnostics, sensitivity analyses, and Monte Carlo standard error calculations for Bayesian simulation studies. Although the BASIS framework focuses primarily on methodological research, it also provides useful guidance for researchers who rely on the results of Bayesian simulation studies or analyses, as current state-of-the-art guidelines for Bayesian analyses are incorporated into the BASIS.},
	author = {Riko Kelter},
	doi = {10.1002/bimj.202200095},
	issn = {15214036},
	issue = {1},
	journal = {Biometrical Journal},
	keywords = {Bayesian simulation study (BASIS) framework,Bayesian statistics,methodological research,reproducibility of research,statistical simulation studies},
	month = {1},
	pmid = {36642811},
	publisher = {John Wiley and Sons Inc},
	title = {The Bayesian simulation study (BASIS) framework for simulation studies in statistical and methodological research},
	volume = {66},
	year = {2024}
}

@article{Koehler2009,
	abstract = {Statistical experiments, more commonly referred to as Monte Carlo or simulation studies, are used to study the behavior of statistical methods and measures under controlled situations. Whereas recent computing and methodological advances have permitted increased efficiency in the simulation process, known as variance reduction, such experiments remain limited by their finite nature and hence are subject to uncertainty; when a simulation is run more than once, different results are obtained. However, virtually no emphasis has been placed on reporting the uncertainty, referred to here as Monte Carlo error, associated with simulation results in the published literature, or on justifying the number of replications used. These deserve broader consideration. Here we present a series of simple and practical methods for estimating Monte Carlo error as well as determining the number of replications required to achieve a desired level of accuracy. The issues and methods are demonstrated with two simple examples, one evaluating operating characteristics of the maximum likelihood estimator for the parameters in logistic regression and the other in the context of using the bootstrap to obtain 95% confidence intervals. The results suggest that in many settings, Monte Carlo error may be more substantial than traditionally thought. © 2009 American Statistical Association.},
	author = {Elizabeth Koehler and Elizabeth Brown and Sebastien J.P.A. Haneuse},
	doi = {10.1198/tast.2009.0030},
	issn = {00031305},
	issue = {2},
	journal = {American Statistician},
	keywords = {Bootstrap,Jackknife,Replication},
	pages = {155-162},
	pmid = {22544972},
	title = {On the assessment of Monte Carlo error in simulation-based Statistical analyses},
	volume = {63},
	year = {2009}
}


@article{Morris2019,
	author = {Tim P. Morris and Ian R. White and Michael J. Crowther},
	doi = {10.1002/sim.8086},
	issn = {10970258},
	issue = {11},
	journal = {Statistics in Medicine},
	keywords = {Monte Carlo,graphics for simulation,simulation design,simulation reporting,simulation studies},
	pages = {2074-2102},
	pmid = {30652356},
	title = {Using simulation studies to evaluate statistical methods},
	volume = {38},
	year = {2019}
}

@article{Thomadakis2019,
	author = {Christos Thomadakis and Loukia Meligkotsidou and Nikos Pantazis and Giota Touloumi},
	doi = {10.1111/biom.12986},
	issn = {15410420},
	issue = {1},
	journal = {Biometrics},
	keywords = {MAR drop-out,MCMC,asymptotic bias,joint modeling,shared random effects models},
	month = {3},
	pages = {58-68},
	pmid = {30357814},
	publisher = {John Wiley and Sons Inc},
	title = {Longitudinal and time-to-drop-out joint models can lead to seriously biased estimates when the drop-out mechanism is at random},
	volume = {75},
	year = {2019}
}
